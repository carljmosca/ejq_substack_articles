# Default Ollama Configuration
quarkus.langchain4j.ollama.timeout=60s

# Default chat model
quarkus.langchain4j.ollama.chat-model.model-id=llama3.2
quarkus.langchain4j.ollama.chat-model.log-responses=true
# Named model: coder
quarkus.langchain4j.ollama.coder.chat-model.model-id=codellama
quarkus.langchain4j.ollama.coder.log-responses=true

# Named model: summarizer
quarkus.langchain4j.ollama.summarizer.chat-model.model-id=mistral
quarkus.langchain4j.ollama.summarizer.log-responses=true

# Embedding model
quarkus.langchain4j.ollama.embedding-model.model-id=nomic-embed-text

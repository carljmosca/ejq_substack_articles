# Researcher 
quarkus.langchain4j.ollama.researcher.chat-model.model-id=llama3.2:3b
quarkus.langchain4j.ollama.researcher.chat-model.temperature=0.8
#quarkus.langchain4j.ollama.researcher.chat-model.num-predict=200
quarkus.langchain4j.ollama.researcher.timeout=60s
quarkus.langchain4j.ollama.researcher.log-requests=true
quarkus.langchain4j.ollama.researcher.log-responses=true

# renderer 
quarkus.langchain4j.ollama.renderer.chat-model.model-id=qwen3:8b
# Lower temperature for more deterministic JSON output
quarkus.langchain4j.ollama.renderer.chat-model.temperature=0.1
#quarkus.langchain4j.ollama.renderer.chat-model.num-predict=100
quarkus.langchain4j.ollama.renderer.chat-model.format=json
quarkus.langchain4j.ollama.renderer.timeout=120s
quarkus.langchain4j.ollama.renderer.log-requests=true
quarkus.langchain4j.ollama.renderer.log-responses=true